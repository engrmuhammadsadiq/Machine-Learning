{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd62e664-f92c-4bdc-8fe9-d2f2c2f42c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\bahijaan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d555e54c-1ee9-44d9-8ac2-a263f322fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\bahijaan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ccd560-0bdc-49ff-8f1c-c4010bdb509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e7b765-0a8b-4fd2-9b2e-87eb63301556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0dd509-ec67-404b-aa49-63554b4489c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3128 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_data',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebaa9c30-0c89-4530-b5ad-3545b1c81ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1189 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'test_data',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9110d7b-5524-42fd-aebd-2c03b203d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c563e477-e5e0-459f-b20d-22f59bc0146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64 , kernel_size=3 , activation='relu' , input_shape=[64,64,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1409b2da-f4ee-48c9-96e7-1dfc247577fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64 , kernel_size=3 , activation='relu' ))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 , strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c71a88-f1ec-4590-8479-222e6bf5bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11292666-3444-4287-82a3-b1fcffbd877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f952f6a-3113-4d02-9fe5-1dc27eb7260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce5710e-cd52-49c1-ac90-73a2afd18c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=5 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cf26c3-c130-432b-a829-9511b9f24ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf5921-33af-48d9-a1e9-564c904077f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c599e2-5075-4815-b0db-dae110c2c245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 68s 676ms/step - loss: 1.3634 - accuracy: 0.4092 - val_loss: 1.1995 - val_accuracy: 0.4987\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 28s 288ms/step - loss: 1.0965 - accuracy: 0.5534 - val_loss: 1.1401 - val_accuracy: 0.5425\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 29s 293ms/step - loss: 1.0016 - accuracy: 0.6087 - val_loss: 1.1306 - val_accuracy: 0.5652\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 32s 322ms/step - loss: 0.9297 - accuracy: 0.6467 - val_loss: 1.1225 - val_accuracy: 0.5652\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 33s 333ms/step - loss: 0.8895 - accuracy: 0.6551 - val_loss: 1.0434 - val_accuracy: 0.5913\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.8408 - accuracy: 0.6793 - val_loss: 1.0586 - val_accuracy: 0.6005\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 36s 367ms/step - loss: 0.7982 - accuracy: 0.6873 - val_loss: 1.1108 - val_accuracy: 0.5971\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 32s 324ms/step - loss: 0.7508 - accuracy: 0.7267 - val_loss: 1.0104 - val_accuracy: 0.6283\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.7131 - accuracy: 0.7318 - val_loss: 1.0583 - val_accuracy: 0.5988\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 31s 314ms/step - loss: 0.6955 - accuracy: 0.7395 - val_loss: 1.0018 - val_accuracy: 0.6392\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 32s 321ms/step - loss: 0.6722 - accuracy: 0.7401 - val_loss: 0.9342 - val_accuracy: 0.6644\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 33s 332ms/step - loss: 0.6393 - accuracy: 0.7577 - val_loss: 1.0433 - val_accuracy: 0.6224\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 29s 293ms/step - loss: 0.6162 - accuracy: 0.7727 - val_loss: 1.1059 - val_accuracy: 0.6291\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 29s 296ms/step - loss: 0.6051 - accuracy: 0.7692 - val_loss: 0.9810 - val_accuracy: 0.6552\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 28s 282ms/step - loss: 0.5935 - accuracy: 0.7820 - val_loss: 0.9262 - val_accuracy: 0.6661\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 31s 318ms/step - loss: 0.5587 - accuracy: 0.7890 - val_loss: 0.9750 - val_accuracy: 0.6611\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 31s 320ms/step - loss: 0.5334 - accuracy: 0.7976 - val_loss: 1.1547 - val_accuracy: 0.6140\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 32s 329ms/step - loss: 0.5160 - accuracy: 0.8053 - val_loss: 0.9958 - val_accuracy: 0.6501\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.5022 - accuracy: 0.8095 - val_loss: 1.0111 - val_accuracy: 0.6535\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 29s 296ms/step - loss: 0.4739 - accuracy: 0.8213 - val_loss: 1.1834 - val_accuracy: 0.6198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1611917aa90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set , validation_data = test_set , epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141f28fe-bd16-4459-ae15-68ee718491d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 216ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('validation_data/tulip2.jpeg',target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a49911-ddd0-4f72-9461-d505799d85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tulip\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    print('Daisy')\n",
    "elif result[0][1]==1:\n",
    "    print('Dandelion')\n",
    "elif result[0][2]==1:\n",
    "    print('Rose')\n",
    "elif result[0][3]==1:\n",
    "    print('SunFlower')\n",
    "elif result[0][4]==1:\n",
    "    print(\"Tulip\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c4afb1d-fdd0-4ada-b9c8-e364311ad4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113f8a7b-820b-469c-986b-161cf3312330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('validation_data/rose.png',target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3bae277-9ab9-4717-af7b-fa0ec7a78e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rose\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    print('Daisy')\n",
    "elif result[0][1]==1:\n",
    "    print('Dandelion')\n",
    "elif result[0][2]==1:\n",
    "    print('Rose')\n",
    "elif result[0][3]==1:\n",
    "    print('SunFlower')\n",
    "elif result[0][4]==1:\n",
    "    print(\"Tulip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed701e-c482-4929-bb55-ea1d3cce962b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
